{"title":"Semantic Similarity","markdown":{"yaml":{"title":"Semantic Similarity","author":"Giordano Rogers","date":"2025-01-04","categories":["NLP","math"],"image":""},"headingText":"Generate random vectors and compute metrics","containsRefs":false,"markdown":"\n\nPart of why natural language processing appeals to me is the prospect of being\nable to compare two bodies of text mathematically.\n\nLately I've been building a an app that generates newsletter summarizations of\nemerging research.\n\nBut I've been asking myself how I can improve the paper filtering.\n\nAs of now, let's say on a given day my app collects 100 papers from my research\nfield.\n\nEven if all of these papers were summarized to a single sentence, I wouldn't want\nto read all that.\n\nSo we need a way to automate the selection process, so that the three to five\npapers I see when I open up my email, are ones that I'm actually interested in.\n\nOne very simply approach would be to keep a list of keywords, and to check if\nany of the papers have abstracts that contain one of my keywords.\n\nAn issue with this is the problem of synonyms.\n\nWhat if I'm very interested in signal processing for music, and I decide to use\nthe keywords\n\n```{python}\ndef keyword_search(\n    abstracts, keywords = [\"signal processing\", \"music\"]\n    ):\n    selected_abstracts = []\n    for abstract in abstracts:\n        for keyword in keywords:\n            if keyword in abstract.strip().lower():\n                selected_abstracts.append(abstract)\n    return selected_abstracts\n```\n\nLet's say an amazing new paper drops introducing a revolutionary new algorithm\nfor musical signal processing, but the abstract only uses the common acronym\nDSP, for digital signal processing, rather than the whole word? What if instead\nof \"music\" they use synonys like \"melody\" or \"instrumental\"?\n\nIn that case, my simple string search method for selection wouldn't work.\n\nAlthough the keyword search technically does fall under the branch of natural\nlanguage processing, it is a very rudimentary approach and we have much better\nways to perform this task.\n\nThe most common method currently is cosine similarity.\n\nThis involves converting two pieces of texts you want to compare into their own\nvector representaitons using LLM embeddings, and then computing the angle \nbetween the two vectors.\n\nThe mathematical formula for cosine similarity:\n\n$$\n\\cos(\\theta) = \\frac{\\mathbf{a} \\cdot \\mathbf{b}}{ \\lVert \\mathbf{a} \\rVert \\lVert \\mathbf{b} \\rVert}\n$$\n\nIn python:\n\n```{python}\nfrom numpy import dot, linalg\n\ndef cosine_similarity(a, b):\n  a_dot_b = dot(a, b)\n  a_norm = linalg.norm(a)\n  b_norm = linalg.norm(b)\n  return a_dot_b / (a_norm * b_norm)\n```\n\nImplementation is stripped down for readability and teaching purposes, but fundamentally this is the essence of cosine similarity. It is surprising how much can be achieved with such a simple function.\n\nAs a toy example, consider we have three simplified vectors that represent three words.\n\n```{python}\nfrom numpy import random\n\ndog = random.normal(9, 0.1, 10)\ncat = random.normal(9, 0.1, 10)\nchocolate = random.normal(-1, 0.1, 10)\n```\n\nIn practice, these vectors would be much longer, and we may be comparing whole paragraphs or documents rather than just single words. But we can look at this example and, both in language and in math, intuit that the first two terms are more similar than to eachother than either is to the third.\n\nUsing our cosing similarity function, we can calcualte this similarity.\n\n```{python}\nfrom numpy import dot, linalg, random\n\ndef cosine_similarity(a, b):\n  a_dot_b = dot(a, b)\n  a_norm = linalg.norm(a)\n  b_norm = linalg.norm(b)\n  return a_dot_b / (a_norm * b_norm)\n\nrandom.seed(69)\ndog = random.normal(9, 0.1, 10)\ncat = random.normal(9, 0.1, 10)\nchocolate = random.normal(-1, 0.1, 10)\n\nprint(\"Similarity between 'cat' and 'dog':\")\nprint(cosine_similarity(cat, dog))\nprint(\"\\nSimilarity between 'cat' and 'chocolate':\")\nprint(cosine_similarity(cat, chocolate))\nprint(\"\\nSimilarity between 'dog' and 'chocolate':\")\nprint(cosine_similarity(cat, chocolate))\n```\n\nIf we extend this logic out to the comparison between research paper abstracts, and my list of keywords, then we wouldn't necessarily need the keywords themselves to be present in the text. All we would need is for the semantic meaning of the words in the text to be close enough to the semantics of my keywords so that the text is selected for as relevant, relative to all the other abstracts I collected on a given day.\n\nBut even though cosine similarity is the most popular tool for this job, it isn't the only way. And maybe not even the best way.\n\nAnother method is to find the Euclidean distance,\n\nIn math notation, this is represented as:\n\n$$\nd = \\sqrt{\\sum_{i=1}^{n}(a_i - b_i)^2}\n$$\n\nAnd in python:\n\n```{python}\nfrom numpy import sqrt\n\ndef euclidean_distance(a, b):\n  sum = 0\n  for a_i, b_i in zip(a, b):\n    sum += (a_i - b_i) ** 2\n  return sqrt(sum)\n```\n\nLet's see how this function performs on our cat, dog, and chocolate vectors:\n\n```{python}\nfrom numpy import dot, linalg, sqrt, random\n\ndef cosine_similarity(a, b):\n  a_dot_b = dot(a, b)\n  a_norm = linalg.norm(a)\n  b_norm = linalg.norm(b)\n  return a_dot_b / (a_norm * b_norm)\n\ndef euclidean_distance(a, b):\n  sum = 0\n  for a_i, b_i in zip(a, b):\n    sum += (a_i - b_i) ** 2\n  return sqrt(sum)\n\nrandom.seed(69)\ndog = random.normal(9, 0.1, 10)\ncat = random.normal(9, 0.1, 10)\nchocolate = random.normal(-1, 0.1, 10)\n\nprint(\"\\nDistance between 'cat' and 'dog':\")\nprint(euclidean_distance(cat, dog))\nprint(\"\\nDistance between 'cat' and 'chocolate':\")\nprint(euclidean_distance(cat, chocolate))\nprint(\"\\nDistance between 'dog' and 'chocolate':\")\nprint(euclidean_distance(cat, chocolate))\n```\n\nAgain this makes sense. We can see that the distance between 'cat' and 'dog' is relatively small, wheras the distances between each word and 'chocolate' is relatively large.\n\nBut so far we've been testing these algorithms on very simple data. Each of these vectors has only ten dimensions.\n\nHow would these algorithms hold up with realistic data? For example, vector embeddings generated with OpenAI's embeddings model create vectors with 1536 dimension.\n\nAgain we'll use a hack to get us there for demonstration purposes. Let's change these ten dimensional vectors into 1500-dimensional vectors.\n\n```{python}\nfrom numpy import dot, linalg, sqrt, random\n\ndef cosine_similarity(a, b):\n  a_dot_b = dot(a, b)\n  a_norm = linalg.norm(a)\n  b_norm = linalg.norm(b)\n  return a_dot_b / (a_norm * b_norm)\n\ndef euclidean_distance(a, b):\n  sum = 0\n  for a_i, b_i in zip(a, b):\n    sum += (a_i - b_i) ** 2\n  return sqrt(sum)\n\nrandom.seed(69)\ndog = random.normal(9, 0.1, 1500)\ncat = random.normal(9, 0.1, 1500)\nchocolate = random.normal(-1, 0.1, 1500)\n\nprint(\"Similarity between 'cat' and 'dog':\")\nprint(cosine_similarity(cat, dog))\nprint(\"\\nSimilarity between 'cat' and 'chocolate':\")\nprint(cosine_similarity(cat, chocolate))\nprint(\"\\nSimilarity between 'dog' and 'chocolate':\")\nprint(cosine_similarity(cat, chocolate))\n\nprint(\"\\nDistance between 'cat' and 'dog':\")\nprint(euclidean_distance(cat, dog))\nprint(\"\\nDistance between 'cat' and 'chocolate':\")\nprint(euclidean_distance(cat, chocolate))\nprint(\"\\nDistance between 'dog' and 'chocolate':\")\nprint(euclidean_distance(cat, chocolate))\n```\n\nNow let's use some statistical methods to see how the quality of our low-dimensional\nsimilarity/distance scores, and our high-dimensional similarity/distance scores stack up.\n\n```{python}\nfrom numpy import (\n  corrcoef, dot, linalg, sqrt,\n  random, std, var, sqrt\n)\n\ndef cosine_similarity(a, b):\n  a_dot_b = dot(a, b)\n  a_norm = linalg.norm(a)\n  b_norm = linalg.norm(b)\n  return a_dot_b / (a_norm * b_norm)\n\ndef euclidean_distance(a, b):\n  sum = 0\n  for a_i, b_i in zip(a, b):\n    sum += (a_i - b_i) ** 2\n  return sqrt(sum)\n\ndef diem(a, b, v_min, v_max, n_dim):\n    ed = sqrt(sum((ai - bi)**2 for ai, bi in zip(a, b)))\n    e_d = sqrt(n_dim) * (v_max - v_min) / 6  # Expected distance\n    sigma_ed = sqrt((v_max - v_min)**2 / 12)\n    return (v_max - v_min) / sigma_ed**2 * (ed - e_d)\n\nrandom.seed(69)\ndog = random.normal(9, 0.1, 1500)\ncat = random.normal(9, 0.1, 1500)\nchocolate = random.normal(-1, 0.1, 1500)\n\ncosine_cat_dog = cosine_similarity(cat, dog)\neuclidean_cat_dog = euclidean_distance(cat, dog)\ndiem_cat_dog = diem(cat, dog, -1, 9, 1500)\n\nprint(f\"Cosine Similarity: {cosine_cat_dog}\")\nprint(f\"Euclidean Distance: {euclidean_cat_dog}\")\nprint(f\"DIEM: {diem_cat_dog}\")\n```","srcMarkdownNoYaml":"\n\nPart of why natural language processing appeals to me is the prospect of being\nable to compare two bodies of text mathematically.\n\nLately I've been building a an app that generates newsletter summarizations of\nemerging research.\n\nBut I've been asking myself how I can improve the paper filtering.\n\nAs of now, let's say on a given day my app collects 100 papers from my research\nfield.\n\nEven if all of these papers were summarized to a single sentence, I wouldn't want\nto read all that.\n\nSo we need a way to automate the selection process, so that the three to five\npapers I see when I open up my email, are ones that I'm actually interested in.\n\nOne very simply approach would be to keep a list of keywords, and to check if\nany of the papers have abstracts that contain one of my keywords.\n\nAn issue with this is the problem of synonyms.\n\nWhat if I'm very interested in signal processing for music, and I decide to use\nthe keywords\n\n```{python}\ndef keyword_search(\n    abstracts, keywords = [\"signal processing\", \"music\"]\n    ):\n    selected_abstracts = []\n    for abstract in abstracts:\n        for keyword in keywords:\n            if keyword in abstract.strip().lower():\n                selected_abstracts.append(abstract)\n    return selected_abstracts\n```\n\nLet's say an amazing new paper drops introducing a revolutionary new algorithm\nfor musical signal processing, but the abstract only uses the common acronym\nDSP, for digital signal processing, rather than the whole word? What if instead\nof \"music\" they use synonys like \"melody\" or \"instrumental\"?\n\nIn that case, my simple string search method for selection wouldn't work.\n\nAlthough the keyword search technically does fall under the branch of natural\nlanguage processing, it is a very rudimentary approach and we have much better\nways to perform this task.\n\nThe most common method currently is cosine similarity.\n\nThis involves converting two pieces of texts you want to compare into their own\nvector representaitons using LLM embeddings, and then computing the angle \nbetween the two vectors.\n\nThe mathematical formula for cosine similarity:\n\n$$\n\\cos(\\theta) = \\frac{\\mathbf{a} \\cdot \\mathbf{b}}{ \\lVert \\mathbf{a} \\rVert \\lVert \\mathbf{b} \\rVert}\n$$\n\nIn python:\n\n```{python}\nfrom numpy import dot, linalg\n\ndef cosine_similarity(a, b):\n  a_dot_b = dot(a, b)\n  a_norm = linalg.norm(a)\n  b_norm = linalg.norm(b)\n  return a_dot_b / (a_norm * b_norm)\n```\n\nImplementation is stripped down for readability and teaching purposes, but fundamentally this is the essence of cosine similarity. It is surprising how much can be achieved with such a simple function.\n\nAs a toy example, consider we have three simplified vectors that represent three words.\n\n```{python}\nfrom numpy import random\n\ndog = random.normal(9, 0.1, 10)\ncat = random.normal(9, 0.1, 10)\nchocolate = random.normal(-1, 0.1, 10)\n```\n\nIn practice, these vectors would be much longer, and we may be comparing whole paragraphs or documents rather than just single words. But we can look at this example and, both in language and in math, intuit that the first two terms are more similar than to eachother than either is to the third.\n\nUsing our cosing similarity function, we can calcualte this similarity.\n\n```{python}\nfrom numpy import dot, linalg, random\n\ndef cosine_similarity(a, b):\n  a_dot_b = dot(a, b)\n  a_norm = linalg.norm(a)\n  b_norm = linalg.norm(b)\n  return a_dot_b / (a_norm * b_norm)\n\nrandom.seed(69)\ndog = random.normal(9, 0.1, 10)\ncat = random.normal(9, 0.1, 10)\nchocolate = random.normal(-1, 0.1, 10)\n\nprint(\"Similarity between 'cat' and 'dog':\")\nprint(cosine_similarity(cat, dog))\nprint(\"\\nSimilarity between 'cat' and 'chocolate':\")\nprint(cosine_similarity(cat, chocolate))\nprint(\"\\nSimilarity between 'dog' and 'chocolate':\")\nprint(cosine_similarity(cat, chocolate))\n```\n\nIf we extend this logic out to the comparison between research paper abstracts, and my list of keywords, then we wouldn't necessarily need the keywords themselves to be present in the text. All we would need is for the semantic meaning of the words in the text to be close enough to the semantics of my keywords so that the text is selected for as relevant, relative to all the other abstracts I collected on a given day.\n\nBut even though cosine similarity is the most popular tool for this job, it isn't the only way. And maybe not even the best way.\n\nAnother method is to find the Euclidean distance,\n\nIn math notation, this is represented as:\n\n$$\nd = \\sqrt{\\sum_{i=1}^{n}(a_i - b_i)^2}\n$$\n\nAnd in python:\n\n```{python}\nfrom numpy import sqrt\n\ndef euclidean_distance(a, b):\n  sum = 0\n  for a_i, b_i in zip(a, b):\n    sum += (a_i - b_i) ** 2\n  return sqrt(sum)\n```\n\nLet's see how this function performs on our cat, dog, and chocolate vectors:\n\n```{python}\nfrom numpy import dot, linalg, sqrt, random\n\ndef cosine_similarity(a, b):\n  a_dot_b = dot(a, b)\n  a_norm = linalg.norm(a)\n  b_norm = linalg.norm(b)\n  return a_dot_b / (a_norm * b_norm)\n\ndef euclidean_distance(a, b):\n  sum = 0\n  for a_i, b_i in zip(a, b):\n    sum += (a_i - b_i) ** 2\n  return sqrt(sum)\n\nrandom.seed(69)\ndog = random.normal(9, 0.1, 10)\ncat = random.normal(9, 0.1, 10)\nchocolate = random.normal(-1, 0.1, 10)\n\nprint(\"\\nDistance between 'cat' and 'dog':\")\nprint(euclidean_distance(cat, dog))\nprint(\"\\nDistance between 'cat' and 'chocolate':\")\nprint(euclidean_distance(cat, chocolate))\nprint(\"\\nDistance between 'dog' and 'chocolate':\")\nprint(euclidean_distance(cat, chocolate))\n```\n\nAgain this makes sense. We can see that the distance between 'cat' and 'dog' is relatively small, wheras the distances between each word and 'chocolate' is relatively large.\n\nBut so far we've been testing these algorithms on very simple data. Each of these vectors has only ten dimensions.\n\nHow would these algorithms hold up with realistic data? For example, vector embeddings generated with OpenAI's embeddings model create vectors with 1536 dimension.\n\nAgain we'll use a hack to get us there for demonstration purposes. Let's change these ten dimensional vectors into 1500-dimensional vectors.\n\n```{python}\nfrom numpy import dot, linalg, sqrt, random\n\ndef cosine_similarity(a, b):\n  a_dot_b = dot(a, b)\n  a_norm = linalg.norm(a)\n  b_norm = linalg.norm(b)\n  return a_dot_b / (a_norm * b_norm)\n\ndef euclidean_distance(a, b):\n  sum = 0\n  for a_i, b_i in zip(a, b):\n    sum += (a_i - b_i) ** 2\n  return sqrt(sum)\n\nrandom.seed(69)\ndog = random.normal(9, 0.1, 1500)\ncat = random.normal(9, 0.1, 1500)\nchocolate = random.normal(-1, 0.1, 1500)\n\nprint(\"Similarity between 'cat' and 'dog':\")\nprint(cosine_similarity(cat, dog))\nprint(\"\\nSimilarity between 'cat' and 'chocolate':\")\nprint(cosine_similarity(cat, chocolate))\nprint(\"\\nSimilarity between 'dog' and 'chocolate':\")\nprint(cosine_similarity(cat, chocolate))\n\nprint(\"\\nDistance between 'cat' and 'dog':\")\nprint(euclidean_distance(cat, dog))\nprint(\"\\nDistance between 'cat' and 'chocolate':\")\nprint(euclidean_distance(cat, chocolate))\nprint(\"\\nDistance between 'dog' and 'chocolate':\")\nprint(euclidean_distance(cat, chocolate))\n```\n\nNow let's use some statistical methods to see how the quality of our low-dimensional\nsimilarity/distance scores, and our high-dimensional similarity/distance scores stack up.\n\n```{python}\nfrom numpy import (\n  corrcoef, dot, linalg, sqrt,\n  random, std, var, sqrt\n)\n\ndef cosine_similarity(a, b):\n  a_dot_b = dot(a, b)\n  a_norm = linalg.norm(a)\n  b_norm = linalg.norm(b)\n  return a_dot_b / (a_norm * b_norm)\n\ndef euclidean_distance(a, b):\n  sum = 0\n  for a_i, b_i in zip(a, b):\n    sum += (a_i - b_i) ** 2\n  return sqrt(sum)\n\ndef diem(a, b, v_min, v_max, n_dim):\n    ed = sqrt(sum((ai - bi)**2 for ai, bi in zip(a, b)))\n    e_d = sqrt(n_dim) * (v_max - v_min) / 6  # Expected distance\n    sigma_ed = sqrt((v_max - v_min)**2 / 12)\n    return (v_max - v_min) / sigma_ed**2 * (ed - e_d)\n\n# Generate random vectors and compute metrics\nrandom.seed(69)\ndog = random.normal(9, 0.1, 1500)\ncat = random.normal(9, 0.1, 1500)\nchocolate = random.normal(-1, 0.1, 1500)\n\ncosine_cat_dog = cosine_similarity(cat, dog)\neuclidean_cat_dog = euclidean_distance(cat, dog)\ndiem_cat_dog = diem(cat, dog, -1, 9, 1500)\n\nprint(f\"Cosine Similarity: {cosine_cat_dog}\")\nprint(f\"Euclidean Distance: {euclidean_cat_dog}\")\nprint(f\"DIEM: {diem_cat_dog}\")\n```"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"highlight-style":"github-dark","output-file":"index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.6.39","theme":["cyborg","../../custom.scss"],"fontsize":"1.3em","monofont":"Courier","title-block-banner":true,"title":"Semantic Similarity","author":"Giordano Rogers","date":"2025-01-04","categories":["NLP","math"],"image":""},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}